{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보안 과제 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### https://pyimagesearch.com/2021/03/01/adversarial-attacks-with-fgsm-fast-gradient-sign-method/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "        # initialize the model along with the input shape\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        # first CONV => RELU => BN layer set\n",
    "        model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",\n",
    "            input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        # second CONV => RELU => BN layer set\n",
    "        model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        # first (and only) set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        # softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        # return the constructed network architecture\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.losses import MSE\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def generate_image_adversary(model, image, label, eps=2 / 255.0):\n",
    "\t# cast the image\n",
    "\timage = tf.cast(image, tf.float32)\n",
    "\t# record our gradients\n",
    "\twith tf.GradientTape() as tape:\n",
    "\t\t# explicitly indicate that our image should be tacked for\n",
    "\t\t# gradient updates\n",
    "\t\ttape.watch(image)\n",
    "\t\t# use our model to make predictions on the input image and\n",
    "\t\t# then compute the loss\n",
    "\t\tpred = model(image)\n",
    "\t\tloss = MSE(label, pred)\n",
    "\t\t# calculate the gradients of loss with respect to the image, then\n",
    "\t\t# compute the sign of the gradient\n",
    "\t\tgradient = tape.gradient(loss, image)\n",
    "\t\tsignedGrad = tf.sign(gradient)\n",
    "\t\t# construct the image adversary\n",
    "\t\tadversary = (image + (signedGrad * eps)).numpy()\n",
    "\t\t# return the image adversary to the calling function\n",
    "\t\treturn adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "# from pyimagesearch.simplecnn import SimpleCNN\n",
    "# from pyimagesearch.fgsm import generate_image_adversary\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# load MNIST dataset and scale the pixel values to the range [0, 1]\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "(trainX, trainY), (testX, testY) = mnist.load_data()\n",
    "trainX = trainX / 255.0\n",
    "testX = testX / 255.0\n",
    "# add a channel dimension to the images\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "# one-hot encode our labels\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/10\n",
      "938/938 [==============================] - 27s 26ms/step - loss: 0.1987 - accuracy: 0.9411 - val_loss: 0.0686 - val_accuracy: 0.9781\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0762 - accuracy: 0.9769 - val_loss: 0.0491 - val_accuracy: 0.9854\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.0567 - accuracy: 0.9829 - val_loss: 0.0372 - val_accuracy: 0.9868\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 20s 21ms/step - loss: 0.0447 - accuracy: 0.9862 - val_loss: 0.0455 - val_accuracy: 0.9855\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.0390 - accuracy: 0.9874 - val_loss: 0.0401 - val_accuracy: 0.9857\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 19s 20ms/step - loss: 0.0350 - accuracy: 0.9889 - val_loss: 0.0374 - val_accuracy: 0.9871\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0280 - accuracy: 0.9910 - val_loss: 0.0412 - val_accuracy: 0.9881\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 21s 22ms/step - loss: 0.0250 - accuracy: 0.9918 - val_loss: 0.1069 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 21s 23ms/step - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0354 - val_accuracy: 0.9892\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 25s 26ms/step - loss: 0.0199 - accuracy: 0.9937 - val_loss: 0.0438 - val_accuracy: 0.9865\n",
      "[INFO] loss: 0.0438, acc: 0.9865\n"
     ]
    }
   ],
   "source": [
    "# initialize our optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=1e-3)\n",
    "model = SimpleCNN.build(width=28, height=28, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the simple CNN on MNIST\n",
    "print(\"[INFO] training network...\")\n",
    "model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=64,\n",
    "\tepochs=10,\n",
    "\tverbose=1)\n",
    "\n",
    "# make predictions on the testing set for the model trained on\n",
    "# non-adversarial images\n",
    "(loss, acc) = model.evaluate(x=testX, y=testY, verbose=0)\n",
    "print(\"[INFO] loss: {:.4f}, acc: {:.4f}\".format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "# loop over a sample of our testing images\n",
    "for i in np.random.choice(np.arange(0, len(testX)), size=(10,)):\n",
    "    # grab the current image and label\n",
    "    image = testX[i]\n",
    "    label = testY[i]\n",
    "    # generate an image adversary for the current image and make\n",
    "    # a prediction on the adversary\n",
    "    adversary = generate_image_adversary(model,\n",
    "        image.reshape(1, 28, 28, 1), label, eps=0.1)\n",
    "    pred = model.predict(adversary)\n",
    "    # scale both the original image and adversary to the range\n",
    "    # [0, 255] and convert them to an unsigned 8-bit integers\n",
    "    adversary = adversary.reshape((28, 28)) * 255\n",
    "    adversary = np.clip(adversary, 0, 255).astype(\"uint8\")\n",
    "    image = image.reshape((28, 28)) * 255\n",
    "    image = image.astype(\"uint8\")\n",
    "    # convert the image and adversarial image from grayscale to three\n",
    "    # channel (so we can draw on them)\n",
    "    image = np.dstack([image] * 3)\n",
    "    adversary = np.dstack([adversary] * 3)\n",
    "    # resize the images so we can better visualize them\n",
    "    image = cv2.resize(image, (96, 96))\n",
    "    adversary = cv2.resize(adversary, (96, 96))\n",
    "    # determine the predicted label for both the original image and\n",
    "    # adversarial image\n",
    "    imagePred = label.argmax()\n",
    "    adversaryPred = pred[0].argmax()\n",
    "    color = (0, 255, 0)\n",
    "    # if the image prediction does not match the adversarial\n",
    "    # prediction then update the color\n",
    "    if imagePred != adversaryPred:\n",
    "        color = (0, 0, 255)\n",
    "    # draw the predictions on the respective output images\n",
    "    cv2.putText(image, str(imagePred), (2, 25),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.95, (0, 255, 0), 2)\n",
    "    cv2.putText(adversary, str(adversaryPred), (2, 25),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.95, color, 2)\n",
    "    # stack the two images horizontally and then show the original\n",
    "    # image and adversarial image\n",
    "    output = np.hstack([image, adversary])\n",
    "    cv2.imshow(\"FGSM Adversarial Images\", output)\n",
    "    cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
