{"cells":[{"cell_type":"markdown","metadata":{},"source":["# <FGSM 적용하기></FGSM>"]},{"cell_type":"markdown","metadata":{},"source":["# **Dependacies**"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import datasets, layers, models\n","import os\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["# **Importing Training Data**"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["ClassNames = ['Aircraft Carrier', 'Bulkers', 'Car Carrier', 'Container Ship', 'Cruise', 'DDG', 'Recreational', 'Sailboat', 'Submarine', 'Tug']"]},{"cell_type":"markdown","metadata":{},"source":["## 파일을 불러오는데 경로를 체크해야함"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["c:\\dev\\workspace\\Asia_Arrived_La\\SON\n"]}],"source":["import os\n","print(os.getcwd())"]},{"cell_type":"markdown","metadata":{},"source":["### 위 코드를 사용하면 c:\\dev\\workspace\\ai_sec\\Asia_Arrived_La\\SON 본인은 이렇게 나옴 각자 자신의 폴더 위치에 따라 다르겠지만\n"," \n","### c:\\dev\\workspace\\ai_sec\\Asia_Arrived_La\\ 해당 부분을 아래 기본 경로 정의에 적어주면 된다. 자기의 폴더 구조에 따라 다를 것이다."]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["Base_Path = r'c:\\dev\\workspace\\Asia_Arrived_La' ## --------> 해당 부분임"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["이미지 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\train\\images\n","라벨 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\train\\labels\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","\n","# 기본 경로 정의\n","base_path = Base_Path  ## --------> 해당 부분임\n","\n","# 기본 경로를 기준으로 이미지와 라벨 경로 정의\n","data_image = os.path.join(base_path, 'Data', 'Ships_dataset', 'train', 'images')\n","data_label = os.path.join(base_path, 'Data', 'Ships_dataset', 'train', 'labels')\n","\n","# 절대 경로로 변환\n","pathi = os.path.abspath(data_image)\n","pathl = os.path.abspath(data_label)\n","\n","# 경로를 확인하기 위해 출력\n","print(\"이미지 경로:\", pathi)\n","print(\"라벨 경로:\", pathl)\n","\n","traindata = []\n","\n","\n","for file in os.listdir(pathi):\n","    file_p  = os.path.join(pathi, file)\n","    image = cv2.imread(file_p)\n","    image = np.array(image)\n","    image=cv2.resize(image, (64,64),interpolation=cv2.INTER_LINEAR)\n","    traindata.append(image) "]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["trainlabel = []\n","for file in os.listdir(pathl):\n","    file_p  = os.path.join(pathl, file)\n","    f = open(file_p, \"r\")\n","    a = int(f.read(1))\n","    trainlabel.append(a)"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["data = []\n","for i in range(0,len(trainlabel)):\n","    traindata[i] = traindata[i]/255\n","    data.append((traindata[i], trainlabel[i]))\n","\n","random.shuffle(data)\n","images, labels =  [], []\n","for a, b in data:\n","    images.append(a)\n","    labels.append(b)\n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["# **Visualize the Data**"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 700x700 with 16 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.figure(figsize=(7, 7))\n","\n","for i in range(16):\n","    plt.subplot(4,4,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.imshow(images[i], cmap=plt.cm.binary)\n","    plt.xlabel(ClassNames[labels[i]])\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# **Importing Validation Data**"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["이미지 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\valid\\images\n","라벨 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\valid\\labels\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","\n","# 기본 경로 정의\n","base_path = Base_Path\n","\n","# 기본 경로를 기준으로 이미지와 라벨 경로 정의\n","data_image = os.path.join(base_path, 'Data', 'Ships_dataset', 'valid', 'images')\n","data_label = os.path.join(base_path, 'Data', 'Ships_dataset', 'valid', 'labels')\n","\n","# 절대 경로로 변환\n","pathi = os.path.abspath(data_image)\n","pathl = os.path.abspath(data_label)\n","\n","\n","\n","# 경로를 확인하기 위해 출력\n","print(\"이미지 경로:\", pathi)\n","print(\"라벨 경로:\", pathl)\n","\n","\n","validdata = []\n","\n","\n","for file in os.listdir(pathi):\n","    file_p  = os.path.join(pathi, file)\n","    image = cv2.imread(file_p)\n","    image = np.array(image)\n","    image=cv2.resize(image, (64,64),interpolation=cv2.INTER_LINEAR)\n","    validdata.append(image)\n","    \n","validlabel = []\n","for file in os.listdir(pathl):\n","    file_p  = os.path.join(pathl, file)\n","    f = open(file_p, \"r\")\n","    a = int(f.read(1))\n","    validlabel.append(a)\n","    \n","validation = []\n","for i in range(0,len(validlabel)):\n","    validdata[i] = validdata[i]/255\n","    validation.append((validdata[i], validlabel[i]))\n","\n","random.shuffle(validation)\n","validationimages, validationlabels =  [], []\n","for a, b in validation:\n","    validationimages.append(a)\n","    validationlabels.append(b)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["images = np.array(images)\n","labels = np.array(labels)\n","validationimages = np.array(validationimages)\n","validationlabels = np.array(validationlabels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# **Model**"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["# model = models.Sequential()\n","# model.add(layers.Conv2D(64,(3,3),activation='relu',input_shape=(64,64,3)))\n","# model.add(layers.MaxPooling2D((2,2)))\n","# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n","# model.add(layers.MaxPooling2D((2,2)))\n","# model.add(layers.Conv2D(128,(3,3),activation='relu'))\n","# model.add(layers.Flatten())\n","# model.add(layers.Dense(128,activation='relu'))\n","# model.add(layers.Dense(64,activation='relu'))\n","# model.add(layers.Dense(10,activation='softmax'))\n","\n","# model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{},"source":["# **Training the Model**"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["# model.fit(images,labels,batch_size = 32, epochs=20, validation_data=(validationimages,validationlabels))\n","\n","# loss, accuracy = model.evaluate(images,labels)\n","# print(f\"Loss: {loss}\")\n","# print(f\"Accuracy: {accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Exporing the Model**"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[],"source":["# model.save('ShipClassifierV1.h5')"]},{"cell_type":"markdown","metadata":{},"source":["# **Loading the model**"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]}],"source":["model = models.load_model('ShipClassifierV1.h5')"]},{"cell_type":"markdown","metadata":{},"source":["# **Importing testing Data**"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["이미지 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\test\\images\n","라벨 경로: c:\\dev\\workspace\\Asia_Arrived_La\\Data\\Ships_dataset\\test\\labels\n"]}],"source":["import os\n","import cv2\n","import numpy as np\n","\n","# 기본 경로 정의\n","base_path = Base_Path\n","\n","# 기본 경로를 기준으로 이미지와 라벨 경로 정의\n","data_image = os.path.join(base_path, 'Data', 'Ships_dataset', 'test', 'images')\n","data_label = os.path.join(base_path, 'Data', 'Ships_dataset', 'test', 'labels')\n","\n","# 절대 경로로 변환\n","pathi = os.path.abspath(data_image)\n","pathl = os.path.abspath(data_label)\n","\n","# 경로를 확인하기 위해 출력\n","print(\"이미지 경로:\", pathi)\n","print(\"라벨 경로:\", pathl)\n","\n","\n","testdata = []\n","\n","\n","for file in os.listdir(pathi):\n","    file_p  = os.path.join(pathi, file)\n","    image = cv2.imread(file_p)\n","    image = np.array(image)\n","    image=cv2.resize(image, (64,64),interpolation=cv2.INTER_LINEAR)\n","    testdata.append(image)\n","    \n","testlabel = []\n","for file in os.listdir(pathl):\n","    file_p  = os.path.join(pathl, file)\n","    f = open(file_p, \"r\")\n","    a = int(f.read(1))\n","    testlabel.append(a)\n","    \n","testing = []\n","for i in range(0,len(testlabel)):\n","    testdata[i] = testdata[i]/255\n","    testing.append((testdata[i], testlabel[i]))\n","\n","\n","\n","random.shuffle(testing)\n","testingimages, testinglabels =  [], []\n","for a, b in testing:\n","    testingimages.append(a)\n","    testinglabels.append(b)\n","    \n","testinglabels = np.array(testinglabels)\n","testingimages = np.array(testingimages)"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('int32')"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["testinglabels.dtype"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('float64')"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["testingimages.dtype"]},{"cell_type":"markdown","metadata":{},"source":["# **Testing the model**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loss, accuracy = model.evaluate(testingimages,testinglabels)\n","# print(f\"Loss: {loss}\")\n","# print(f\"Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for i in range(4):\n","#     plt.subplot(2,2,i+1)\n","#     plt.xticks([])\n","#     plt.yticks([])\n","#     plt.imshow(testingimages[i],cmap=plt.cm.binary)\n","#     pred = model.predict(np.array([testingimages[i]]))\n","#     index = np.argmax(pred)\n","#     plt.xlabel(f\"Actual = {ClassNames[testinglabels[i]]} \\n Predicted = {ClassNames[index]}\")\n","    \n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## baseline model FGSM 공격 결과"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 0.8525 - loss: 1.0786\n","원본 테스트 데이터에서의 손실: 1.0981699228286743\n","원본 테스트 데이터에서의 정확도: 0.847769021987915\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.7798 - loss: 4.4630\n","적대적 테스트 데이터에서의 손실: 5.095647811889648\n","적대적 테스트 데이터에서의 정확도: 0.7690288424491882\n"]}],"source":["# import the necessary packages\n","from tensorflow.keras.losses import MSE\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_adversary(model, image, label, eps=2 / 255.0):\n","    image = tf.cast(image, tf.float32)\n","    with tf.GradientTape() as tape:\n","        tape.watch(image)\n","        pred = model(image)\n","        loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        adversary = (image + (signedGrad * eps)).numpy()\n","        return adversary\n","        ## return 적대적 이미지 \n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = model.evaluate(testingimages, testinglabels)\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n","\n","\n","\n","# 적대적 예제를 저장할 리스트\n","adversarial_examples = []\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성\n","for i in range(len(testingimages)):\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    adversary = generate_image_adversary(model, image.reshape(1, 64, 64, 3), label, eps=0.1)\n","    adversarial_examples.append(adversary)\n","\n","\n","# 적대적 예제를 numpy 배열로 변환\n","adversarial_examples = np.vstack(adversarial_examples)\n","\n","\n","\n","# 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, testinglabels)\n","print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["## baseline model BIM 공격 결과"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8525 - loss: 1.0786\n","원본 테스트 데이터에서의 손실: 1.0981699228286743\n","원본 테스트 데이터에서의 정확도: 0.847769021987915\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.4293 - loss: 48.6022\n","BIM 적대적 테스트 데이터에서의 손실: 50.35498046875\n","BIM 적대적 테스트 데이터에서의 정확도: 0.4278215169906616\n"]}],"source":["def generate_image_bim(model, image, label, eps = 2 / 255.0, alpha=0.1, iterations=10):\n","    image = tf.cast(image, tf.float32)\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape:\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","      \n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = model.evaluate(testingimages, testinglabels)\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n","\n","# 적대적 예제를 저장할 리스트\n","bim_adversarial_examples = []\n","\n","\n","# 테스트 이미지에 대해 BIM 적대적 예제 생성\n","for i in range(len(testingimages)) :\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    bim_adversary = generate_image_bim(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.1, iterations=10)\n","    bim_adversarial_examples.append(bim_adversary)\n","\n","\n","\n","\n","# 적대적 예제를 numpy 배열로 변환\n","bim_adversarial_examples = np.vstack(bim_adversarial_examples)\n","\n","\n","\n","\n","# BIM 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","bim_loss, bim_accuracy = model.evaluate(bim_adversarial_examples, testinglabels)\n","print(f\"BIM 적대적 테스트 데이터에서의 손실: {bim_loss}\")\n","print(f\"BIM 적대적 테스트 데이터에서의 정확도: {bim_accuracy}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## baseline model PGD 공격 결과"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.8525 - loss: 1.0786\n","원본 테스트 데이터에서의 손실: 1.0981699228286743\n","원본 테스트 데이터에서의 정확도: 0.847769021987915\n","\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.6715 - loss: 8.5595\n","PGD 적대적 테스트 데이터에서의 손실: 9.197137832641602\n","PGD 적대적 테스트 데이터에서의 정확도: 0.6377952694892883\n"]}],"source":["def generate_image_pgd(model, image, label, eps=2 / 255.0, alpha=0.1, iterations=10):\n","    image = tf.cast(image, tf.float32)\n","    original_image = image\n","    perturbation = tf.random.uniform(image.shape, -eps, eps, dtype=tf.float32)\n","    image = image + perturbation\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape:\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","        # 클리핑을 통해 이미지 값이 원래 범위를 넘지 않도록 보장\n","        image = tf.clip_by_value(image, original_image - eps, original_image + eps)\n","        image = tf.clip_by_value(image, 0, 1)  # 이미지 값이 0과 1 사이에 있도록 보장\n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = model.evaluate(testingimages, testinglabels)\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n","\n","\n","pgd_adversarial_examples = []\n","\n","\n","# 테스트 이미지에 대해 PGD 적대적 예제 생성\n","for i in range(len(testingimages)):\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    pgd_adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.1, iterations=10)\n","    pgd_adversarial_examples.append(pgd_adversary)\n","\n","\n","pgd_adversarial_examples = np.vstack(pgd_adversarial_examples)\n","\n","\n","# PGD 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","pgd_loss, pgd_accuracy = model.evaluate(pgd_adversarial_examples, testinglabels)\n","print(f\"PGD 적대적 테스트 데이터에서의 손실: {pgd_loss}\")\n","print(f\"PGD 적대적 테스트 데이터에서의 정확도: {pgd_accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["## baseline model DEEPFOOL 공격 결과"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8525 - loss: 1.0786\n","원본 테스트 데이터에서의 손실: 1.0981699228286743\n","원본 테스트 데이터에서의 정확도: 0.847769021987915\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.1056 - loss: 21.2618\n","PGD 적대적 테스트 데이터에서의 손실: 20.58628273010254\n","PGD 적대적 테스트 데이터에서의 정확도: 0.12598425149917603\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10):\n","    image = tf.cast(image, tf.float32)\n","    image_label = tf.argmax(model(image), 1).numpy()[0]\n","\n","    pert_image = image\n","    r_tot = tf.zeros_like(image)\n","    loop_i = 0\n","    x = tf.Variable(pert_image)\n","    w = tf.zeros_like(image)\n","\n","    fs = model(x)\n","    k_i = image_label  # k_i 초기화\n","\n","    while k_i == image_label and loop_i < max_iter:\n","        pert = np.inf\n","        with tf.GradientTape() as tape:\n","            tape.watch(x)\n","            fs = model(x)\n","            loss = fs[0][k_i]\n","        grad_orig = tape.gradient(loss, x)\n","\n","        for k in range(num_classes):\n","            if k == k_i:\n","                continue\n","            \n","            with tf.GradientTape() as tape:\n","                tape.watch(x)\n","                fs = model(x)\n","                loss = fs[0][k]\n","            cur_grad = tape.gradient(loss, x)\n","\n","            w_k = cur_grad - grad_orig\n","            f_k = (fs[0][k] - fs[0][k_i]).numpy()\n","            pert_k = abs(f_k) / tf.norm(tf.reshape(w_k, [-1]))\n","            \n","            if pert_k < pert:\n","                pert = pert_k\n","                w = w_k\n","\n","        r_i = pert * tf.sign(w)\n","        r_tot = r_tot + r_i\n","        pert_image = image + (1 + overshoot) * r_tot\n","        x.assign(pert_image)\n","\n","        loop_i += 1\n","        k_i = tf.argmax(model(x), 1).numpy()[0]\n","\n","    # 적대적 이미지를 [0, 1] 범위로 클리핑\n","    clipped_adversarial_image = tf.clip_by_value(pert_image, clip_value_min=0, clip_value_max=1)\n","    return clipped_adversarial_image.numpy()\n","\n","#\n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = model.evaluate(testingimages, testinglabels)\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n","\n","\n","DEEPFOOL_adversarial_examples = []\n","\n","\n","# 테스트 이미지에 대해 deepfool 적대적 예제 생성\n","for i in range(len(testingimages)):\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    DEEPFOOL_adversary= deepfool(image.reshape(1, 64, 64, 3), model, num_classes=10, overshoot=0.02, max_iter=10)\n","    # DEEPFOOL_adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.01, iterations=10)\n","    DEEPFOOL_adversarial_examples.append(DEEPFOOL_adversary)\n","\n","\n","DEEPFOOL_adversarial_examples = np.vstack(DEEPFOOL_adversarial_examples)\n","\n","\n","# PGD 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","DEEPFOLSL, DEEPFOOL_accuracy = model.evaluate(DEEPFOOL_adversarial_examples, testinglabels)\n","print(f\"PGD 적대적 테스트 데이터에서의 손실: {DEEPFOLSL}\")\n","print(f\"PGD 적대적 테스트 데이터에서의 정확도: {DEEPFOOL_accuracy}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## train 이미지 생성"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.losses import MSE\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_original(model, image, label, eps= 2 / 255.0, alpha=0 ):\n","    image = tf.cast(image, tf.float32)\n","    with tf.GradientTape() as tape:\n","        tape.watch(image)\n","        pred = model(image)\n","        loss = MSE(label, pred)\n","    gradient = tape.gradient(loss, image)\n","    signedGrad = tf.sign(gradient)\n","    image = image + alpha * signedGrad\n","      \n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","# 저장 경로 설정\n","base_path = \"./original_adversary_images\"  # Modify with your actual path\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)  # Ensure the image directory exists\n","os.makedirs(label_save_path, exist_ok=True)  # Ensure the label directory exists\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성 및 저장\n","for i in range(7400):  # Assuming you have at least 2000 images in your test set\n","    image = images[i]\n","    label = labels[i]\n","    adversary = generate_image_original(model, image.reshape(1, 64, 64, 3), label, eps=0, alpha=0)\n","    \n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])\n","    \n","    # 라벨 저장\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file:\n","        label_file.write(str(label))\n","\n","\n","\n","# 적대적 예제를 리스트에서 numpy 배열로 변환\n","adversarial_examples = np.vstack(adversarial_examples)\n","selected_labels = np.array(selected_labels)\n","\n","\n","\n","\n","# # 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## fgsm으로 적대적 이미지 2000개 만들어서 저장"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.losses import MSE\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_adversary(model, image, label, eps=2 / 255.0) :\n","    image = tf.cast(image, tf.float32)\n","    with tf.GradientTape() as tape :\n","        tape.watch(image)\n","        pred = model(image)\n","        loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        adversary = (image + (signedGrad * eps)).numpy()\n","        return adversary\n","\n","\n","\n","# 저장 경로 설정\n","base_path = \"./fgsm_adversary_images\"  # Modify with your actual path\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)  # Ensure the image directory exists\n","os.makedirs(label_save_path, exist_ok=True)  # Ensure the label directory exists\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성 및 저장\n","for i in range(2000) :  # Assuming you have at least 2000 images in your test set\n","    image = images[i]\n","    label = labels[i]\n","    adversary = generate_image_adversary(model, image.reshape(1, 64, 64, 3), label, eps=0.1)\n","    \n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])\n","    \n","    # 라벨 저장\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file:\n","        label_file.write(str(label))\n","\n","\n","\n","# # 적대적 예제를 리스트에서 numpy 배열로 변환\n","# adversarial_examples = np.vstack(adversarial_examples)\n","# selected_labels = np.array(selected_labels)\n","\n","\n","\n","\n","# # 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## BIM 적대적 이미지 2000개 생성"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.losses import MSE\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_bim(model, image, label, eps=2 / 255.0, alpha=0.01, iterations=10) :\n","    image = tf.cast(image, tf.float32)\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape :\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","      \n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","# 저장 경로 설정\n","base_path = \"./BIM_adversary_images\"  # Modify with your actual path\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)  # Ensure the image directory exists\n","os.makedirs(label_save_path, exist_ok=True)  # Ensure the label directory exists\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성 및 저장\n","for i in range(2000) :  # Assuming you have at least 2000 images in your test set\n","    image = images[i]\n","    label = labels[i]\n","    adversary = generate_image_bim(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.01, iterations=10)\n","    \n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])\n","    \n","    # 라벨 저장\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file:\n","        label_file.write(str(label))\n","\n","\n","\n","# # 적대적 예제를 리스트에서 numpy 배열로 변환\n","# adversarial_examples = np.vstack(adversarial_examples)\n","# selected_labels = np.array(selected_labels)\n","\n","\n","\n","\n","# # 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["##  PGD 적대적 이미지 생성"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def generate_image_pgd(model, image, label, eps=2 / 255.0, alpha=0.01, iterations=10):\n","    image = tf.cast(image, tf.float32)\n","    original_image = image\n","    perturbation = tf.random.uniform(image.shape, -eps, eps, dtype=tf.float32)\n","    image = image + perturbation\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape:\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","        # 클리핑을 통해 이미지 값이 원래 범위를 넘지 않도록 보장\n","        image = tf.clip_by_value(image, original_image - eps, original_image + eps)\n","        image = tf.clip_by_value(image, 0, 1)  # 이미지 값이 0과 1 사이에 있도록 보장\n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","# 저장 경로 설정\n","base_path = \"./PGD_adversary_images\"  # Modify with your actual path\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)  # Ensure the image directory exists\n","os.makedirs(label_save_path, exist_ok=True)  # Ensure the label directory exists\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","# 테스트 이미지에 대해 적대적 예제 생성 및 저장\n","for i in range(2000):  # Assuming you have at least 2000 images in your test set\n","    image = images[i]\n","    label = labels[i]\n","    adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.01, iterations=10)\n","    \n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])\n","    \n","    # 라벨 저장\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file :\n","        label_file.write(str(label))\n","\n","\n","\n","# # 적대적 예제를 리스트에서 numpy 배열로 변환\n","# adversarial_examples = np.vstack(adversarial_examples)\n","# selected_labels = np.array(selected_labels)\n","\n","# # 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Deepfool 적대적 이미지 생성"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:99: RuntimeWarning: invalid value encountered in cast\n","  return pil_image.fromarray(x.astype(\"uint8\"), \"RGB\")\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10):\n","    image = tf.cast(image, tf.float32)\n","    image_label = tf.argmax(model(image), 1).numpy()[0]\n","\n","    pert_image = image\n","    r_tot = tf.zeros_like(image)\n","    loop_i = 0\n","    x = tf.Variable(pert_image)\n","    w = tf.zeros_like(image)\n","\n","    fs = model(x)\n","    k_i = image_label  # k_i 초기화\n","\n","    while k_i == image_label and loop_i < max_iter:\n","        pert = np.inf\n","        with tf.GradientTape() as tape:\n","            tape.watch(x)\n","            fs = model(x)\n","            loss = fs[0][k_i]\n","        grad_orig = tape.gradient(loss, x)\n","\n","        for k in range(num_classes):\n","            if k == k_i:\n","                continue\n","            \n","            with tf.GradientTape() as tape :\n","                tape.watch(x)\n","                fs = model(x)\n","                loss = fs[0][k]\n","            cur_grad = tape.gradient(loss, x)\n","\n","            w_k = cur_grad - grad_orig\n","            f_k = (fs[0][k] - fs[0][k_i]).numpy()\n","            pert_k = abs(f_k) / tf.norm(tf.reshape(w_k, [-1]))\n","            \n","            if pert_k < pert :\n","                pert = pert_k\n","                w = w_k\n","\n","        r_i = pert * tf.sign(w)\n","        r_tot = r_tot + r_i\n","        pert_image = image + (1 + overshoot) * r_tot\n","        x.assign(pert_image)\n","\n","        loop_i += 1\n","        k_i = tf.argmax(model(x), 1).numpy()[0]\n","\n","    # 적대적 이미지를 [0, 1] 범위로 클리핑\n","    clipped_adversarial_image = tf.clip_by_value(pert_image, clip_value_min=0, clip_value_max=1)\n","    return clipped_adversarial_image.numpy()\n","\n","# 설정한 저장 경로\n","base_path = \"./deepfool_adversary_images\"\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)\n","os.makedirs(label_save_path, exist_ok=True)\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","# 테스트 이미지에 대해 적대적 예제 생성 및 저장\n","for i in range(min(2000, len(images))):  # 이미지 수가 2000개 이하일 수 있으므로 min 함수 사용\n","    image_tensor = tf.reshape(images[i], (1, 64, 64, 3))\n","    image_tensor = tf.cast(image_tensor, tf.float32)  # 데이터 타입을 float32로 조정\n","    label = labels[i]\n","\n","    adversary = deepfool(image_tensor, model, num_classes=10, overshoot=0.02, max_iter=10)\n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 클리핑된 적대적 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])  # adversary[0]를 바로 저장하도록 설정\n","\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file:\n","        label_file.write(str(label))\n","\n","\n","\n","\n","\n","\n","\n","# # 적대적 예제를 리스트에서 numpy 배열로 변환\n","# adversarial_examples = np.vstack(adversarial_examples)\n","# selected_labels = np.array(selected_labels)\n","\n","# # 모델을 사용하여 적대적 예제의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Test 이미지 생성"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.losses import MSE\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","# 적대적 예제 생성 함수\n","def generate_image_original(model, image, label, eps=2/255.0, alpha=0):\n","    image = tf.cast(image, tf.float32)\n","    with tf.GradientTape() as tape:\n","        tape.watch(image)\n","        pred = model(image)\n","        loss = MSE(tf.one_hot([label], depth=10), pred)  # 예를 들어, 클래스가 10개인 경우\n","    gradient = tape.gradient(loss, image)\n","    signed_grad = tf.sign(gradient)\n","    image = image + alpha * signed_grad\n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","# 저장 경로 설정\n","base_path = \"./test_original_adversary_images\"  # 경로 수정 가능\n","image_save_path = os.path.join(base_path, 'images')\n","label_save_path = os.path.join(base_path, 'labels')\n","os.makedirs(image_save_path, exist_ok=True)\n","os.makedirs(label_save_path, exist_ok=True)\n","\n","adversarial_examples = []\n","selected_labels = []\n","\n","# 테스트 이미지와 레이블을 가정합니다. 예: testing_images, testing_labels\n","# for i in range(len(testing_images)):\n","for i in range(380):  # 테스트 세트의 이미지 수에 맞게 조정\n","    image = testingimages[i]\n","    label = testinglabels[i]  # 정수 레이블\n","    adversary = generate_image_original(model, image.reshape(1, 64, 64, 3), label, eps=0, alpha=0)\n","    \n","    adversarial_examples.append(adversary)\n","    selected_labels.append(label)\n","\n","    # 이미지 저장\n","    image_file_path = os.path.join(image_save_path, f'adversary_{i}.png')\n","    save_img(image_file_path, adversary[0])\n","\n","    # 레이블 저장\n","    label_file_path = os.path.join(label_save_path, f'label_{i}.txt')\n","    with open(label_file_path, 'w') as label_file:\n","        label_file.write(str(label))\n","\n","\n","\n","# test 이미지를 변수명 변경하기\n","testingimages = adversarial_examples\n","testinglabels = selected_labels\n","\n","\n","\n","\n","# # 적대적 예제를 사용하여 모델의 손실과 정확도 평가\n","# adversarial_loss, adversarial_accuracy = model.evaluate(adversarial_examples, selected_labels)\n","# print(f\"적대적 테스트 데이터에서의 손실: {adversarial_loss}\")\n","# print(f\"적대적 테스트 데이터에서의 정확도: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('float32')"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["testingimages.dtype"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('float64')"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["testinglabels.dtype"]},{"cell_type":"markdown","metadata":{},"source":["## 데이터 결합 "]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Total images: (15400, 64, 64, 3)\n","Total labels: (15400, 10)\n","Images loaded from ./deepfool_adversary_images: 2000\n","Images loaded from ./fgsm_adversary_images: 2000\n","Images loaded from ./bim_adversary_images: 2000\n","Images loaded from ./PGD_adversary_images: 2000\n","Images loaded from ./original_adversary_images: 7400\n"]}],"source":["import os\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.utils import to_categorical\n","\n","def load_and_merge_datasets(base_paths, image_size=(64, 64), num_classes=None):\n","    all_images = []\n","    all_labels = []\n","    dataset_counts = {}  # 데이터 세트별 이미지 수를 추적하는 딕셔너리\n","    empty_label_count = 0  # 빈 label이 몇 개인지 카운트하는 변수\n","    empty_label_files = []  # 빈 label 파일 경로 저장 리스트\n","\n","    for base_path in base_paths:\n","        image_save_path = os.path.join(base_path, 'images')\n","        label_save_path = os.path.join(base_path, 'labels')\n","        \n","        image_files = sorted([os.path.join(image_save_path, f) for f in os.listdir(image_save_path) if f.endswith('.png')])\n","        label_files = sorted([os.path.join(label_save_path, f) for f in os.listdir(label_save_path) if f.endswith('.txt')])\n","        \n","        dataset_counts[base_path] = len(image_files)  # 현재 데이터 세트의 이미지 수 카운트\n","\n","        for img_file, lbl_file in zip(image_files, label_files):\n","            img = load_img(img_file, target_size=image_size)\n","            img = img_to_array(img)\n","            img /= 255.0\n","\n","            with open(lbl_file, 'r') as file:\n","                label = file.read().strip()\n","\n","                # 레이블이 빈 경우 체크\n","                if not label:\n","                    empty_label_count += 1\n","                    empty_label_files.append(lbl_file)\n","                    continue  # 빈 레이블인 경우 해당 항목 건너뜀\n","\n","                if label.startswith('['):\n","                    label = np.fromstring(label[1:-1], sep=' ')\n","                    if len(label) == 0:  # label이 빈 배열인지 확인\n","                        empty_label_count += 1\n","                        empty_label_files.append(lbl_file)\n","                        continue  # 빈 레이블인 경우 해당 항목 건너뜀\n","                    if len(label.shape) == 1 and num_classes is not None:\n","                        label = to_categorical(int(label[0]), num_classes=num_classes)\n","                else:\n","                    try:\n","                        label = int(label)\n","                        if num_classes is not None:\n","                            label = to_categorical(label, num_classes=num_classes)\n","                    except ValueError:\n","                        empty_label_count += 1\n","                        empty_label_files.append(lbl_file)\n","                        continue  # 레이블이 int로 변환되지 않으면 건너뜀\n","\n","            all_images.append(img)\n","            all_labels.append(label)\n","\n","    all_images = np.array(all_images)\n","    all_labels = np.array(all_labels)\n","\n","\n","    # 빈 label이 있는 경우 출력\n","    if empty_label_count > 0:\n","        print(f\"\\nTotal empty labels: {empty_label_count}\")\n","        print(\"Files with empty labels:\")\n","        for file in empty_label_files:\n","            print(f\"- {file}\")\n","\n","    return all_images, all_labels, dataset_counts\n","\n","\n","\n","# 데이터 세트 경로 리스트\n","base_paths = [\n","    \"./deepfool_adversary_images\",\n","    \"./fgsm_adversary_images\",\n","    \"./bim_adversary_images\",\n","    \"./PGD_adversary_images\",\n","    \"./original_adversary_images\"\n","]\n","\n","\n","num_classes = 10  # 데이터 세트에 따라 적절한 클래스 수 설정\n","\n","# 함수 호출\n","all_adv_images, all_adv_labels, counts = load_and_merge_datasets(base_paths, num_classes=num_classes)\n","\n","print(f\"\\nTotal images: {all_adv_images.shape}\")\n","print(f\"Total labels: {all_adv_labels.shape}\")\n","for path, count in counts.items():\n","    print(f\"Images loaded from {path}: {count}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## shape 확인 필요할"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["(15400, 64, 64, 3)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["all_adv_images.shape"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/plain":["(15400, 10)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["all_adv_labels.shape"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/plain":["dtype('float32')"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["all_adv_images.dtype"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## test 이미지 불러오기"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total images: (380, 64, 64, 3)\n","Total labels: (380, 10)\n","Images loaded from ./test_original_adversary_images: 380\n"]}],"source":["import os\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","\n","def load_test_datasets(base_paths, image_size=(64, 64), num_classes=None):\n","    all_images = []\n","    all_labels = []\n","    dataset_counts = {}  # 데이터 세트별 이미지 수를 추적하는 딕셔너리\n","    empty_label_count = 0  # 빈 label이 몇 개인지 카운트하는 변수\n","    empty_label_files = []  # 빈 label 파일 경로 저장 리스트\n","\n","    for base_path in base_paths:\n","        image_save_path = os.path.join(base_path, 'images')\n","        label_save_path = os.path.join(base_path, 'labels')\n","        \n","        image_files = sorted([os.path.join(image_save_path, f) for f in os.listdir(image_save_path) if f.endswith('.png')])\n","        label_files = sorted([os.path.join(label_save_path, f) for f in os.listdir(label_save_path) if f.endswith('.txt')])\n","        \n","        dataset_counts[base_path] = len(image_files)  # 현재 데이터 세트의 이미지 수 카운트\n","\n","        for img_file, lbl_file in zip(image_files, label_files):\n","            img = load_img(img_file, target_size=image_size)\n","            img = img_to_array(img)\n","            img /= 255.0\n","\n","            with open(lbl_file, 'r') as file:\n","                label = file.read().strip()\n","\n","                # 레이블이 빈 경우 체크\n","                if not label:\n","                    empty_label_count += 1\n","                    empty_label_files.append(lbl_file)\n","                    continue  # 빈 레이블인 경우 해당 항목 건너뜀\n","\n","                if label.startswith('['):\n","                    label = np.fromstring(label[1:-1], sep=' ')\n","                    if len(label) == 0:  # label이 빈 배열인지 확인\n","                        empty_label_count += 1\n","                        empty_label_files.append(lbl_file)\n","                        continue  # 빈 레이블인 경우 해당 항목 건너뜀\n","                    if len(label.shape) == 1 and num_classes is not None:\n","                        label = to_categorical(int(label[0]), num_classes=num_classes)\n","                else:\n","                    try:\n","                        label = int(label)\n","                        if num_classes is not None:\n","                            label = to_categorical(label, num_classes=num_classes)\n","                    except ValueError:\n","                        empty_label_count += 1\n","                        empty_label_files.append(lbl_file)\n","                        continue  # 레이블이 int로 변환되지 않으면 건너뜀\n","\n","            all_images.append(img)\n","            all_labels.append(label)\n","\n","    all_images = np.array(all_images)\n","    all_labels = np.array(all_labels)\n","\n","\n","    # 빈 label이 있는 경우 출력\n","    if empty_label_count > 0:\n","        print(f\"\\nTotal empty labels: {empty_label_count}\")\n","        print(\"Files with empty labels:\")\n","        for file in empty_label_files:\n","            print(f\"- {file}\")\n","\n","    return all_images, all_labels, dataset_counts\n","\n","\n","\n","\n","\n","# 데이터 세트 경로 리스트\n","base_paths = [\n","    \"./test_original_adversary_images\",\n","    \n","]\n","\n","\n","\n","\n","num_classes = 10  # 데이터 세트에 따라 적절한 클래스 수 설정\n","\n","\n","\n","\n","\n","# 함수 호출\n","testingimages, testinglabels, counts = load_test_datasets(base_paths, num_classes=num_classes)\n","\n","\n","\n","\n","\n","print(f\"Total images: {testingimages.shape}\")\n","print(f\"Total labels: {testinglabels.shape}\")\n","for path, count in counts.items() :\n","    print(f\"Images loaded from {path}: {count}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["testingimages shape  + (380, 64, 64, 3)\n","testinglabels shape  + (380, 10)\n","all_adv_images shape  + (15400, 64, 64, 3)\n","all_adv_labels shape  +  (15400, 10)\n"]}],"source":["print(\"testingimages shape  +\" , testingimages.shape)\n","print(\"testinglabels shape  +\" , testinglabels.shape)\n","print(\"all_adv_images shape  +\", all_adv_images.shape)\n","print(\"all_adv_labels shape  + \" , all_adv_labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### train 이미지 7400개와 적대적 이미지 8000개를 통합 한 images, labels "]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 236ms/step - accuracy: 0.4470 - loss: 1.6325\n","Epoch 2/5\n","\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 241ms/step - accuracy: 0.8788 - loss: 0.3983\n","Epoch 3/5\n","\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 244ms/step - accuracy: 0.9400 - loss: 0.1859\n","Epoch 4/5\n","\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 243ms/step - accuracy: 0.9705 - loss: 0.0963\n","Epoch 5/5\n","\u001b[1m482/482\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 239ms/step - accuracy: 0.9747 - loss: 0.0721\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.8341 - loss: 0.7613\n","원본 테스트 데이터에서의 손실: 0.8417989015579224\n","원본 테스트 데이터에서의 정확도: 0.8394736647605896\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import layers, models\n","\n","\n","\n","# 증강된 데이터를 사용하여 모델을 재학습하는 함수\n","def retrain_model(model, images, labels, epochs=5):\n","    model.fit(images, labels, epochs=epochs, batch_size=32)\n","\n","\n","\n","\n","# 모델 정의\n","num_classes = 10\n","model = models.Sequential([\n","    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(128, (3, 3), activation='relu'),\n","    layers.Flatten(),\n","    layers.Dense(128, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(num_classes, activation='softmax')\n","])\n","\n","\n","\n","# 모델 컴파일\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","\n","# 모델 재학습\n","retrain_model(model, all_adv_images, all_adv_labels)\n","\n","\n","\n","\n","# 모델 저장\n","model.save('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","\n","# 모델 로드 및 성능 평가\n","upgrade_model = models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = upgrade_model.evaluate(testingimages, testinglabels)\n","\n","\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 적대적 이미지 비율에 ㄷ라느 모델의 성능 그때 강화학습을 사용하는거지."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## fgsm을 공격을 적용한 이미지를 새로 학습한 모델로 분류"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 89ms/step - accuracy: 0.8207 - loss: 1.0995\n","Adversarial Test Data Loss: 1.206473708152771\n","Adversarial Test Data Accuracy: 0.8110235929489136\n"]}],"source":["from tensorflow.keras.losses import MSE\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_adversaries(model, image, label, eps = 2 / 255.0) :\n","    image = tf.cast(image, tf.float32)\n","    with tf.GradientTape() as tape :\n","        tape.watch(image)\n","        pred = model(image)\n","        loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signed_grad = tf.sign(gradient)\n","        adversary = (image + (signed_grad * eps)).numpy()\n","        return adversary\n","\n","\n","\n","# 적대적 예제를 저장할 리스트\n","adversarial_examples = []\n","\n","\n","\n","model = models.load_model('ShipClassifierV1.h5')\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성\n","for i in range(len(testingimages)) :\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    adversary = generate_image_adversaries(model, image.reshape(1, 64, 64, 3), label, eps=0.1)\n","    adversarial_examples.append(adversary)\n","\n","\n","\n","# 적대적 예제를 numpy 배열로 변환\n","adversarial_examples = np.vstack(adversarial_examples)\n","\n","\n","\n","# Ensure testinglabels are one-hot encoded\n","if testinglabels.ndim == 1 :  # assuming testinglabels are currently integer class labels\n","    testinglabels_onehot = tf.keras.utils.to_categorical(testinglabels, num_classes=10)\n","else:\n","    testinglabels_onehot = testinglabels\n","\n","\n","\n","# Load the upgraded model\n","upgrade_model = tf.keras.models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","# Recompile the model with the necessary loss function, optimizer, and metrics\n","upgrade_model.compile(optimizer='adam', \n","                      loss='categorical_crossentropy',  # or whatever loss function is appropriate\n","                      metrics=['accuracy'])\n","\n","\n","# Evaluate the model using the adversarial examples and the corrected label format\n","adversarial_loss, adversarial_accuracy = upgrade_model.evaluate(adversarial_examples, testinglabels_onehot)\n","print(f\"Adversarial Test Data Loss: {adversarial_loss}\")\n","print(f\"Adversarial Test Data Accuracy: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## BIM 공격을 적용한 적대적 이미지 새로 학습한 모델로 분류"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 120ms/step - accuracy: 0.2377 - loss: 17.7893\n","Adversarial Test Data Loss: 17.60909652709961\n","Adversarial Test Data Accuracy: 0.23622047901153564\n"]}],"source":["from tensorflow.keras.losses import MSE\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","\n","\n","# 적대적 예제 생성 함수\n","def generate_image_bim(model, image, label, eps=2 / 255.0, alpha=0.1, iterations=10) :\n","    image = tf.cast(image, tf.float32)\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape :\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","      \n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","\n","# 적대적 예제를 저장할 리스트\n","adversarial_examples = []\n","\n","\n","\n","model = models.load_model('ShipClassifierV1.h5')\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성\n","for i in range(len(testingimages)) :\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    bim_adversary = generate_image_bim(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.1, iterations=10)\n","    adversarial_examples.append(adversary)\n","\n","\n","\n","# 적대적 예제를 numpy 배열로 변환\n","adversarial_examples = np.vstack(adversarial_examples)\n","\n","\n","\n","\n","# Ensure testinglabels are one-hot encoded\n","if testinglabels.ndim == 1 :  # assuming testinglabels are currently integer class labels\n","    testinglabels_onehot = tf.keras.utils.to_categorical(testinglabels, num_classes=10)\n","else :\n","    testinglabels_onehot = testinglabels\n","\n","\n","\n","\n","# Load the upgraded model\n","upgrade_model = tf.keras.models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","\n","\n","# Recompile the model with the necessary loss function, optimizer, and metrics\n","upgrade_model.compile(optimizer='adam', \n","                      loss='categorical_crossentropy',  # or whatever loss function is appropriate\n","                      metrics=['accuracy'])\n","\n","\n","\n","\n","# Evaluate the model using the adversarial examples and the corrected label format\n","adversarial_loss, adversarial_accuracy = upgrade_model.evaluate(adversarial_examples, testinglabels_onehot)\n","print(f\"Adversarial Test Data Loss: {adversarial_loss}\")\n","print(f\"Adversarial Test Data Accuracy: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## PGD공격을 적용한 적대적 이미지 새로 학습한 모델로 분류"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7927 - loss: 1.3181\n","Adversarial Test Data Loss: 1.4650360345840454\n","Adversarial Test Data Accuracy: 0.7559055089950562\n"]}],"source":["from tensorflow.keras.losses import MSE\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","\n","\n","def generate_image_pgd(model, image, label, eps=2 / 255.0, alpha=0.1, iterations=10):\n","    image = tf.cast(image, tf.float32)\n","    original_image = image\n","    perturbation = tf.random.uniform(image.shape, -eps, eps, dtype=tf.float32)\n","    image = image + perturbation\n","    for i in range(iterations):\n","        with tf.GradientTape() as tape:\n","            tape.watch(image)\n","            pred = model(image)\n","            loss = MSE(label, pred)\n","        gradient = tape.gradient(loss, image)\n","        signedGrad = tf.sign(gradient)\n","        image = image + alpha * signedGrad\n","        # 클리핑을 통해 이미지 값이 원래 범위를 넘지 않도록 보장\n","        image = tf.clip_by_value(image, original_image - eps, original_image + eps)\n","        image = tf.clip_by_value(image, 0, 1)  # 이미지 값이 0과 1 사이에 있도록 보장\n","    adversary = image.numpy()\n","    return adversary\n","\n","\n","\n","\n","# 적대적 예제를 저장할 리스트\n","adversarial_examples = []\n","\n","\n","\n","model = models.load_model('ShipClassifierV1.h5')\n","\n","\n","\n","# 테스트 이미지에 대해 적대적 예제 생성\n","for i in range(len(testingimages)) :\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.1, iterations=10)\n","    adversarial_examples.append(adversary)\n","\n","\n","\n","# 적대적 예제를 numpy 배열로 변환\n","adversarial_examples = np.vstack(adversarial_examples)\n","\n","\n","\n","# Ensure testinglabels are one-hot encoded\n","if testinglabels.ndim == 1 :  # assuming testinglabels are currently integer class labels\n","    testinglabels_onehot = tf.keras.utils.to_categorical(testinglabels, num_classes=10)\n","else:\n","    testinglabels_onehot = testinglabels\n","\n","\n","\n","# Load the upgraded model\n","upgrade_model = tf.keras.models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","# Recompile the model with the necessary loss function, optimizer, and metrics\n","upgrade_model.compile(optimizer='adam', \n","                      loss='categorical_crossentropy',  # or whatever loss function is appropriate\n","                      metrics=['accuracy'])\n","\n","\n","# Evaluate the model using the adversarial examples and the corrected label format\n","adversarial_loss, adversarial_accuracy = upgrade_model.evaluate(adversarial_examples, testinglabels_onehot)\n","print(f\"Adversarial Test Data Loss: {adversarial_loss}\")\n","print(f\"Adversarial Test Data Accuracy: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## DeepFool 공격을 적용한 적대적 이미지 새로 학습한 모델로 분류"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["# from tensorflow.keras.losses import MSE\n","# import tensorflow as tf\n","# import numpy as np\n","\n","\n","\n","# def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10):\n","#     image = tf.cast(image, tf.float32)\n","#     image_label = tf.argmax(model(image), 1).numpy()[0]\n","\n","#     pert_image = image\n","#     r_tot = tf.zeros_like(image)\n","#     loop_i = 0\n","#     x = tf.Variable(pert_image)\n","#     w = tf.zeros_like(image)\n","\n","#     fs = model(x)\n","#     k_i = image_label  # k_i 초기화\n","\n","#     while k_i == image_label and loop_i < max_iter:\n","#         pert = np.inf\n","#         with tf.GradientTape() as tape:\n","#             tape.watch(x)\n","#             fs = model(x)\n","#             loss = fs[0][k_i]\n","#         grad_orig = tape.gradient(loss, x)\n","\n","#         for k in range(num_classes):\n","#             if k == k_i:\n","#                 continue\n","            \n","#             with tf.GradientTape() as tape:\n","#                 tape.watch(x)\n","#                 fs = model(x)\n","#                 loss = fs[0][k]\n","#             cur_grad = tape.gradient(loss, x)\n","\n","#             w_k = cur_grad - grad_orig\n","#             f_k = (fs[0][k] - fs[0][k_i]).numpy()\n","#             pert_k = abs(f_k) / tf.norm(tf.reshape(w_k, [-1]))\n","            \n","#             if pert_k < pert:\n","#                 pert = pert_k\n","#                 w = w_k\n","\n","#         r_i = pert * tf.sign(w)\n","#         r_tot = r_tot + r_i\n","#         pert_image = image + (1 + overshoot) * r_tot\n","#         x.assign(pert_image)\n","\n","#         loop_i += 1\n","#         k_i = tf.argmax(model(x), 1).numpy()[0]\n","\n","#     # 적대적 이미지를 [0, 1] 범위로 클리핑\n","#     clipped_adversarial_image = tf.clip_by_value(pert_image, clip_value_min=0, clip_value_max=1)\n","#     return clipped_adversarial_image.numpy()\n","\n","\n","\n","# # 적대적 예제를 저장할 리스트\n","# adversarial_examples = []\n","\n","\n","\n","# model = models.load_model('ShipClassifierV1.h5')\n","\n","\n","\n","# # 테스트 이미지에 대해 적대적 예제 생성\n","# for i in range(len(testingimages)) :\n","#     image = testingimages[i]\n","#     label = testinglabels[i]\n","#     adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.01, iterations=10)\n","#     adversarial_examples.append(adversary)\n","\n","\n","\n","# # 적대적 예제를 numpy 배열로 변환\n","# adversarial_examples = np.vstack(adversarial_examples)\n","\n","\n","\n","# # Ensure testinglabels are one-hot encoded\n","# if testinglabels.ndim == 1 :  # assuming testinglabels are currently integer class labels\n","#     testinglabels_onehot = tf.keras.utils.to_categorical(testinglabels, num_classes=10)\n","# else:\n","#     testinglabels_onehot = testinglabels\n","\n","\n","\n","# # Load the upgraded model\n","# upgrade_model = tf.keras.models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","# # Recompile the model with the necessary loss function, optimizer, and metrics\n","# upgrade_model.compile(optimizer='adam', \n","#                       loss='categorical_crossentropy',  # or whatever loss function is appropriate\n","#                       metrics=['accuracy'])\n","\n","\n","# # Evaluate the model using the adversarial examples and the corrected label format\n","# adversarial_loss, adversarial_accuracy = upgrade_model.evaluate(adversarial_examples, testinglabels_onehot)\n","# print(f\"Adversarial Test Data Loss: {adversarial_loss}\")\n","# print(f\"Adversarial Test Data Accuracy: {adversarial_accuracy}\")\n"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.8525 - loss: 1.0786\n","원본 테스트 데이터에서의 손실: 1.0981699228286743\n","원본 테스트 데이터에서의 정확도: 0.847769021987915\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.7298 - loss: 1.5417\n","Adversarial Test Data Loss: 1.4264715909957886\n","Adversarial Test Data Accuracy: 0.7349081635475159\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","from tensorflow.keras.preprocessing.image import save_img\n","\n","def deepfool(image, model, num_classes=10, overshoot=0.02, max_iter=10):\n","    image = tf.cast(image, tf.float32)\n","    image_label = tf.argmax(model(image), 1).numpy()[0]\n","\n","    pert_image = image\n","    r_tot = tf.zeros_like(image)\n","    loop_i = 0\n","    x = tf.Variable(pert_image)\n","    w = tf.zeros_like(image)\n","\n","    fs = model(x)\n","    k_i = image_label  # k_i 초기화\n","\n","    while k_i == image_label and loop_i < max_iter:\n","        pert = np.inf\n","        with tf.GradientTape() as tape:\n","            tape.watch(x)\n","            fs = model(x)\n","            loss = fs[0][k_i]\n","        grad_orig = tape.gradient(loss, x)\n","\n","        for k in range(num_classes):\n","            if k == k_i:\n","                continue\n","            \n","            with tf.GradientTape() as tape:\n","                tape.watch(x)\n","                fs = model(x)\n","                loss = fs[0][k]\n","            cur_grad = tape.gradient(loss, x)\n","\n","            w_k = cur_grad - grad_orig\n","            f_k = (fs[0][k] - fs[0][k_i]).numpy()\n","            pert_k = abs(f_k) / tf.norm(tf.reshape(w_k, [-1]))\n","            \n","            if pert_k < pert:\n","                pert = pert_k\n","                w = w_k\n","\n","        r_i = pert * tf.sign(w)\n","        r_tot = r_tot + r_i\n","        pert_image = image + (1 + overshoot) * r_tot\n","        x.assign(pert_image)\n","\n","        loop_i += 1\n","        k_i = tf.argmax(model(x), 1).numpy()[0]\n","\n","    # 적대적 이미지를 [0, 1] 범위로 클리핑\n","    clipped_adversarial_image = tf.clip_by_value(pert_image, clip_value_min=0, clip_value_max=1)\n","    return clipped_adversarial_image.numpy()\n","\n","#\n","\n","\n","\n","# 원본 테스트 데이터에서 모델의 손실과 정확도 평가\n","loss, accuracy = model.evaluate(testingimages, testinglabels)\n","print(f\"원본 테스트 데이터에서의 손실: {loss}\")\n","print(f\"원본 테스트 데이터에서의 정확도: {accuracy}\")\n","\n","\n","DEEPFOOL_adversarial_examples = []\n","\n","\n","# 테스트 이미지에 대해 deepfool 적대적 예제 생성\n","for i in range(len(testingimages)):\n","    image = testingimages[i]\n","    label = testinglabels[i]\n","    DEEPFOOL_adversary= deepfool(image.reshape(1, 64, 64, 3), model, num_classes=10, overshoot=0.02, max_iter=10)\n","    # DEEPFOOL_adversary = generate_image_pgd(model, image.reshape(1, 64, 64, 3), label, eps=0.1, alpha=0.01, iterations=10)\n","    DEEPFOOL_adversarial_examples.append(DEEPFOOL_adversary)\n","\n","\n","DEEPFOOL_adversarial_examples = np.vstack(DEEPFOOL_adversarial_examples)\n","\n","\n","# Ensure testinglabels are one-hot encoded\n","if testinglabels.ndim == 1 :  # assuming testinglabels are currently integer class labels\n","    testinglabels_onehot = tf.keras.utils.to_categorical(testinglabels, num_classes=10)\n","else:\n","    testinglabels_onehot = testinglabels\n","\n","\n","\n","# Load the upgraded model\n","upgrade_model = tf.keras.models.load_model('every_upgrade_ShipClassifierV1.h5')\n","\n","\n","# Recompile the model with the necessary loss function, optimizer, and metrics\n","upgrade_model.compile(optimizer='adam', \n","                      loss='categorical_crossentropy',  # or whatever loss function is appropriate\n","                      metrics=['accuracy'])\n","\n","\n","# Evaluate the model using the adversarial examples and the corrected label format\n","adversarial_loss, adversarial_accuracy = upgrade_model.evaluate(DEEPFOOL_adversarial_examples, testinglabels_onehot)\n","print(f\"Adversarial Test Data Loss: {adversarial_loss}\")\n","print(f\"Adversarial Test Data Accuracy: {adversarial_accuracy}\")\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["##  bim이랑 pgd 성능 더 낮게 만들 예정 "]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2331073,"sourceId":3926270,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
